# Model Inference Testing Guide

This document explains how to test and verify AI model inference in EMBODIOS, comparing it with standard llama.cpp implementation.

## Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
pip install llama-cpp-python
pip install -e .
```

### 2. Verify Model Available

The TinyLlama 1.1B model should be in:
```
models/tinyllama/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
```

If not present, download it:
```bash
embodi pull TinyLlama/TinyLlama-1.1B-Chat-v1.0
```

### 3. Run Comparison Test

```bash
python test_model_comparison.py
```

This will:
- Load the model with llama.cpp
- Run test prompts
- Measure performance
- Generate evidence report
- Save results to `inference_comparison_results.json`

## Test Files

### `test_model_comparison.py`

Automated comparison test that:
- Tests llama.cpp inference with TinyLlama 1.1B
- Simulates EMBODIOS kernel inference architecture
- Measures load time, inference time, tokens/second
- Generates JSON results file
- Creates performance comparison report

### `INFERENCE_COMPARISON_EVIDENCE.md`

Comprehensive evidence report containing:
- Test methodology and configuration
- llama.cpp performance metrics
- EMBODIOS architecture description
- Comparison analysis
- Evidence files and verification steps

### `inference_comparison_results.json`

Raw test data in JSON format:
```json
{
  "implementation": "llama.cpp",
  "prompt": "What is 15 plus 27?",
  "output": "Answer: 42...",
  "load_time_seconds": 0.059,
  "inference_time_seconds": 1.121,
  "tokens_generated": 43,
  "tokens_per_second": 38.35
}
```

## Continuous Integration

### GitHub Actions Workflow

The CI workflow `.github/workflows/model-inference-test.yml` automatically:

1. **Verifies Dependencies**
   - Checks llama-cpp-python installation
   - Verifies embodi package imports
   - Confirms all dependencies available

2. **Checks AI Implementation Files**
   - Verifies `kernel/ai/quantized_inference.c` exists
   - Verifies `kernel/ai/simd_ops.c` exists
   - Verifies `kernel/ai/gguf_integer_loader.c` exists
   - Verifies `kernel/ai/tinyllama_integer_inference.c` exists

3. **Tests Kernel Builds**
   - Builds x86_64 kernel with AI inference
   - Builds ARM64 kernel with AI inference
   - Verifies both builds succeed
   - Uploads kernel artifacts

4. **Runs Inference Tests** (if model available)
   - Executes comparison test script
   - Validates outputs
   - Generates reports

### Triggering CI Tests

The workflow runs on:
- Push to `main` or `feat/**` branches
- Pull requests to `main`
- Manual trigger via GitHub Actions UI
- Changes to AI-related files

## Testing EMBODIOS Kernel Inference

### Building the Kernel

```bash
cd kernel

# For ARM64 (Raspberry Pi)
make ARCH=aarch64

# For x86_64 (PC/QEMU)
make ARCH=x86_64
```

### Running on Hardware

#### Option 1: Raspberry Pi 4

1. Build ARM64 kernel
2. Copy `embodios.bin` to SD card boot partition
3. Configure `config.txt` to load kernel
4. Boot Raspberry Pi
5. Connect via UART/serial console
6. Test prompts directly in kernel

#### Option 2: QEMU Emulation

```bash
# x86_64
qemu-system-x86_64 -kernel kernel/embodios.elf -nographic

# ARM64
qemu-system-aarch64 -M virt -cpu cortex-a57 -kernel kernel/embodios.elf -nographic
```

### Expected Behavior

On boot, you should see:
```
[BOOT] EMBODIOS v0.2.0 Starting...
[BOOT] Memory: 256MB heap initialized
[BOOT] AI Model: TinyLlama 1.1B loaded
[READY] Type commands or prompts

>
```

Then you can type prompts:
```
> Hello, what is your name?
AI: [Response generated by kernel inference engine]

> What is 15 plus 27?
AI: 42
```

## Comparison Metrics

### llama.cpp (User-Space)

**Advantages:**
- GPU acceleration (Metal, CUDA, ROCm)
- Easier debugging
- Full OS services
- Better absolute performance on high-end hardware

**Measured Performance:**
- Load time: 4-17 seconds (first load)
- Inference: 35-67 tokens/second
- Backend: Metal (Apple GPU)

### EMBODIOS (Bare-Metal)

**Advantages:**
- No OS overhead
- Sub-second boot time
- Deterministic latency
- Lower power consumption
- Suitable for embedded systems

**Architecture:**
- Integer-only Q16.16 fixed-point
- ARM NEON SIMD vectorization
- Direct hardware access
- 256MB dedicated heap

## Troubleshooting

### Model Not Found

```bash
# Download model
embodi pull TinyLlama/TinyLlama-1.1B-Chat-v1.0

# Or manually place at:
# models/tinyllama/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
```

### llama-cpp-python Build Issues

On macOS with Apple Silicon:
```bash
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
```

On Linux:
```bash
pip install llama-cpp-python
```

### Kernel Build Failures

Check that you have:
- `build-essential` (Linux)
- `nasm` (for x86_64)
- `gcc-aarch64-linux-gnu` (for ARM64 cross-compile)

```bash
# Ubuntu/Debian
sudo apt-get install build-essential nasm gcc-aarch64-linux-gnu
```

## Running Tests Locally

### Full Comparison Test

```bash
python test_model_comparison.py
```

### Integration Tests

```bash
pytest tests/integration/test_real_model_inference.py -v
```

### Manual Interactive Test

```bash
python tests/integration/test_real_model_inference.py
```

This runs an interactive REPL where you can chat with the model.

## Evidence Files

All test runs generate evidence files:

1. **inference_comparison_results.json**
   - Raw performance data
   - Prompts and outputs
   - Timing measurements

2. **INFERENCE_COMPARISON_EVIDENCE.md**
   - Full test report
   - Methodology description
   - Comparison analysis
   - Next steps

3. **CI Artifacts**
   - Kernel binaries with AI inference
   - Build logs
   - Test output

## Verification Checklist

- [ ] llama.cpp loads TinyLlama model
- [ ] llama.cpp generates coherent responses
- [ ] Performance metrics are reasonable (30-70 tokens/sec)
- [ ] EMBODIOS kernel builds successfully (x86_64)
- [ ] EMBODIOS kernel builds successfully (ARM64)
- [ ] All AI inference source files present
- [ ] CI tests pass
- [ ] Evidence files generated

## Next Steps

To complete full verification:

1. **Deploy to Hardware**
   - Boot EMBODIOS kernel on Raspberry Pi 4
   - Or run in QEMU emulation

2. **Run Identical Prompts**
   - Use same test prompts
   - Compare outputs
   - Measure performance

3. **Document Results**
   - Record response quality
   - Measure boot-to-inference latency
   - Compare power consumption

4. **Generate Final Report**
   - Side-by-side comparison
   - Performance analysis
   - Use case recommendations

## References

- [EMBODIOS Architecture](ARCHITECTURE_COMPARISON.md)
- [Quantized Inference Documentation](docs/quantized-integer-inference.md)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [TinyLlama Model](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
